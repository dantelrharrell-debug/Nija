name: Canary Deployment with Automated Rollback

on:
  push:
    branches: [ main ]
    tags:
      - 'v*'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      canary_percentage:
        description: 'Initial canary traffic percentage'
        required: true
        default: '10'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build-and-scan:
    name: Build and Security Scan
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      security-events: write
    
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-tag: ${{ steps.meta.outputs.tags }}
      
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha,prefix={{branch}}-

    - name: Build and push Docker image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        build-args: |
          GIT_COMMIT=${{ github.sha }}
          GIT_BRANCH=${{ github.ref_name }}
          BUILD_TIMESTAMP=${{ github.event.head_commit.timestamp }}

    # Scan image for vulnerabilities
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ steps.meta.outputs.tags }}
        format: 'sarif'
        output: 'trivy-results.sarif'
        severity: 'CRITICAL,HIGH'
      continue-on-error: true

    - name: Upload Trivy results to GitHub Security
      uses: github/codeql-action/upload-sarif@v3
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'
      continue-on-error: true

    # Sign the container image
    - name: Install Cosign
      uses: sigstore/cosign-installer@v3

    - name: Sign container image
      run: |
        cosign sign --yes ${{ steps.meta.outputs.tags }}@${{ steps.build.outputs.digest }}
      env:
        COSIGN_EXPERIMENTAL: 1

  deploy-canary:
    name: Deploy Canary
    runs-on: ubuntu-latest
    needs: build-and-scan
    environment: 
      name: ${{ github.event.inputs.environment || 'staging' }}
      
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up kubectl
      uses: azure/setup-kubectl@v3

    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > kubeconfig
        echo "KUBECONFIG=$(pwd)/kubeconfig" >> $GITHUB_ENV

    # Deploy canary version
    - name: Deploy Canary Version
      run: |
        export CANARY_IMAGE="${{ needs.build-and-scan.outputs.image-tag }}"
        export CANARY_PERCENTAGE="${{ github.event.inputs.canary_percentage || '10' }}"
        
        # Apply canary deployment
        envsubst < k8s/canary/deployment-canary.yaml | kubectl apply -f -
        
        # Update traffic split
        envsubst < k8s/canary/traffic-split.yaml | kubectl apply -f -
        
        # Wait for canary to be ready
        kubectl rollout status deployment/nija-canary -n nija --timeout=5m

    # Health check for canary
    - name: Canary Health Check
      id: health-check
      run: |
        python scripts/canary_health_check.py \
          --namespace nija \
          --deployment nija-canary \
          --duration 300 \
          --error-threshold 5
      continue-on-error: true

    # Automated rollback if health check fails
    - name: Rollback Canary on Failure
      if: steps.health-check.outcome == 'failure'
      run: |
        echo "::error::Canary health check failed - initiating rollback"
        kubectl delete deployment nija-canary -n nija
        kubectl apply -f k8s/canary/traffic-split-stable.yaml
        exit 1

    # Store canary metrics
    - name: Collect Canary Metrics
      if: always()
      run: |
        python scripts/collect_canary_metrics.py \
          --namespace nija \
          --deployment nija-canary \
          --output canary-metrics.json
      continue-on-error: true

    - name: Upload Canary Metrics
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: canary-metrics
        path: canary-metrics.json

  progressive-rollout:
    name: Progressive Traffic Shift
    runs-on: ubuntu-latest
    needs: deploy-canary
    environment: 
      name: ${{ github.event.inputs.environment || 'staging' }}
    
    strategy:
      matrix:
        traffic_percentage: [25, 50, 75, 100]
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up kubectl
      uses: azure/setup-kubectl@v3

    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > kubeconfig
        echo "KUBECONFIG=$(pwd)/kubeconfig" >> $GITHUB_ENV

    # Shift traffic gradually
    - name: Shift Traffic to ${{ matrix.traffic_percentage }}%
      run: |
        export CANARY_PERCENTAGE="${{ matrix.traffic_percentage }}"
        envsubst < k8s/canary/traffic-split.yaml | kubectl apply -f -
        sleep 60

    # Monitor metrics at each stage
    - name: Monitor Canary at ${{ matrix.traffic_percentage }}%
      id: monitor
      run: |
        python scripts/canary_health_check.py \
          --namespace nija \
          --deployment nija-canary \
          --duration 180 \
          --error-threshold 5 \
          --latency-threshold-ms 1000
      continue-on-error: true

    # Rollback if issues detected
    - name: Rollback on Issues at ${{ matrix.traffic_percentage }}%
      if: steps.monitor.outcome == 'failure'
      run: |
        echo "::error::Issues detected at ${{ matrix.traffic_percentage }}% - rolling back"
        kubectl apply -f k8s/canary/traffic-split-stable.yaml
        kubectl delete deployment nija-canary -n nija
        exit 1

  finalize-deployment:
    name: Finalize Deployment
    runs-on: ubuntu-latest
    needs: progressive-rollout
    environment: 
      name: ${{ github.event.inputs.environment || 'staging' }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up kubectl
      uses: azure/setup-kubectl@v3

    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > kubeconfig
        echo "KUBECONFIG=$(pwd)/kubeconfig" >> $GITHUB_ENV

    # Promote canary to stable
    - name: Promote Canary to Stable
      run: |
        # Update stable deployment with canary image
        kubectl set image deployment/nija-stable \
          nija=${{ needs.build-and-scan.outputs.image-tag }} \
          -n nija
        
        # Wait for stable deployment
        kubectl rollout status deployment/nija-stable -n nija --timeout=5m
        
        # Remove canary deployment
        kubectl delete deployment nija-canary -n nija
        
        # Restore 100% traffic to stable
        kubectl apply -f k8s/canary/traffic-split-stable.yaml

    - name: Deployment Success Notification
      run: |
        echo "::notice::Canary deployment successfully promoted to stable"
